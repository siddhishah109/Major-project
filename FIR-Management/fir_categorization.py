# -*- coding: utf-8 -*-
"""FIr_Categorization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M_oMTnnaO4zMPl2uM8eK1OntjRh1fDgx
"""

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import transformers
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import os
from datetime import datetime
import h5py

class FIRDataset(Dataset):
    """Custom dataset for FIR classification"""
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

def load_training_data():
    """
    Load FIR training data - in a real scenario, this would load from a database
    For this example, we'll create synthetic data
    """
    # Create synthetic training data
    categories = ['theft', 'assault', 'fraud', 'cyber', 'sexual', 'domestic']

    # Example descriptions for each category
    example_descriptions = {
        'theft': [
            "My wallet was stolen from my bag while shopping",
            "Someone broke into my car and took my laptop",
            "My bicycle was stolen from outside my house",
            "Armed robbery at the convenience store",
            "Home break-in, jewelry and electronics taken"
        ],
        'assault': [
            "I was attacked by a stranger on the street",
            "Physical altercation outside a bar",
            "My neighbor punched me during an argument",
            "I was threatened with a weapon",
            "Fight at a sporting event resulted in injuries"
        ],
        'fraud': [
            "Unauthorized transactions on my credit card",
            "Someone used my identity to open new accounts",
            "Received counterfeit currency as change",
            "Investment scam that cost me my savings",
            "Fake charity solicitation"
        ],
        'cyber': [
            "My email was hacked and used to send spam",
            "Ransomware attack on my computer",
            "Online scam through a fake shopping website",
            "Someone created a fake profile using my photos",
            "Phishing attempt to steal my banking information"
        ],
        'sexual': [
            "Inappropriate touching on public transport",
            "Stalking and harassment of sexual nature",
            "Received unsolicited explicit messages",
            "Sexual harassment at workplace",
            "Unwanted advances and threats"
        ],
        'domestic': [
            "Argument with spouse turned violent",
            "Child neglect by parent",
            "Threats from family member",
            "Verbal abuse and intimidation at home",
            "Property damage during domestic dispute"
        ]
    }

    # Expand each category with more examples by adding qualifiers
    qualifiers = [
        "serious incident of", "minor case of", "reported", "alleged",
        "witnessed", "recurring", "first-time", "suspicious"
    ]

    data = []
    for category, examples in example_descriptions.items():
        for example in examples:
            data.append((example, category))
            # Add variations with qualifiers
            for qualifier in qualifiers:
                data.append((f"{qualifier} {example.lower()}", category))

    # Convert to DataFrame
    df = pd.DataFrame(data, columns=['description', 'category'])

    # Add some random variations to make the dataset more realistic
    df['description'] = df['description'].apply(
        lambda x: x + " " + np.random.choice([
            "on Monday", "yesterday", "last week",
            "around midnight", "in the morning",
            "", "", ""  # empty strings to keep some unchanged
        ])
    )

    return df

def train_and_save_model(model_name='bert-base-uncased', output_path='fir_classification_model.h5'):
    """
    Train the BERT model and save it in h5 format
    """
    # Set device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # Load training data
    print("Loading training data...")
    df = load_training_data()
    print(f"Loaded {len(df)} training examples")

    # Encode categories
    label_encoder = LabelEncoder()
    df['encoded_category'] = label_encoder.fit_transform(df['category'])

    # Save label encoding mapping for later use
    label_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}
    print("Category mapping:", label_mapping)

    # Split data
    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)
    print(f"Training set: {len(train_df)}, Validation set: {len(val_df)}")

    # Initialize tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Prepare datasets
    train_dataset = FIRDataset(
        texts=train_df['description'].tolist(),
        labels=train_df['encoded_category'].tolist(),
        tokenizer=tokenizer
    )

    val_dataset = FIRDataset(
        texts=val_df['description'].tolist(),
        labels=val_df['encoded_category'].tolist(),
        tokenizer=tokenizer
    )

    # Create data loaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=8,
        shuffle=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=8,
        shuffle=False
    )

    # Initialize model
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=len(label_mapping)
    )
    model = model.to(device)

    # Define optimizer and loss
    optimizer = AdamW(model.parameters(), lr=2e-5)

    # Training loop
    epochs = 3
    best_accuracy = 0

    for epoch in range(epochs):
        print(f"\nEpoch {epoch+1}/{epochs}")

        # Training phase
        model.train()
        train_loss = 0

        for batch in train_loader:
            # Move batch to device
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            # Forward pass
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )

            loss = outputs.loss
            train_loss += loss.item()

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        avg_train_loss = train_loss / len(train_loader)
        print(f"Average training loss: {avg_train_loss:.4f}")

        # Evaluation phase
        model.eval()
        val_loss = 0
        predictions = []
        true_labels = []

        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )

                loss = outputs.loss
                val_loss += loss.item()

                # Get predictions
                preds = torch.argmax(outputs.logits, dim=1)
                predictions.extend(preds.cpu().tolist())
                true_labels.extend(labels.cpu().tolist())

        avg_val_loss = val_loss / len(val_loader)
        accuracy = sum(1 for x, y in zip(predictions, true_labels) if x == y) / len(predictions)

        print(f"Validation loss: {avg_val_loss:.4f}")
        print(f"Validation accuracy: {accuracy:.4f}")

        # Save best model
        if accuracy > best_accuracy:
            best_accuracy = accuracy

            # Save model state dict
            torch.save(model.state_dict(), 'best_model_state.pt')
            print("Saved best model state")

    # Load best model
    model.load_state_dict(torch.load('best_model_state.pt'))

    # Save model architecture and weights in h5 format
    print(f"Saving model to {output_path}...")

    # Create directory if it doesn't exist
    os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)

    # Save model using h5py
    with h5py.File(output_path, 'w') as h5_file:
        # Save model configuration
        config_group = h5_file.create_group('config')
        config_str = model.config.to_json_string()
        config_group.attrs['config_str'] = config_str
        config_group.attrs['model_name'] = model_name

        # Save model state dict
        state_dict = model.state_dict()
        weights_group = h5_file.create_group('weights')

        for key, value in state_dict.items():
            weights_group.create_dataset(key, data=value.cpu().numpy())

        # Save label encoder mapping
        label_group = h5_file.create_group('labels')
        for idx, label in label_mapping.items():
            label_group.attrs[str(idx)] = label

    print(f"Model saved successfully to {output_path}")

    # Evaluate the final model
    model.eval()
    all_predictions = []
    all_labels = []

    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels']

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )

            preds = torch.argmax(outputs.logits, dim=1)
            all_predictions.extend(preds.cpu().tolist())
            all_labels.extend(labels.tolist())

    # Convert numerical predictions back to category names
    prediction_labels = [label_mapping[pred] for pred in all_predictions]
    true_labels = [label_mapping[label] for label in all_labels]

    # Print classification report
    print("\nClassification Report:")
    print(classification_report(true_labels, prediction_labels))

    return output_path

def load_model_from_h5(model_path, model_name='bert-base-uncased'):
    """
    Load model from h5 file
    """
    # Initialize tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Load h5 file
    with h5py.File(model_path, 'r') as h5_file:
        # Load config
        config_str = h5_file['config'].attrs['config_str']
        config = transformers.AutoConfig.from_pretrained(model_name)

        # Load label mapping
        label_mapping = {}
        for key, value in h5_file['labels'].attrs.items():
            label_mapping[int(key)] = value

        # Initialize model
        model = AutoModelForSequenceClassification.from_pretrained(
            model_name,
            config=config,
            num_labels=len(label_mapping)
        )

        # Load weights
        state_dict = {}
        for key in h5_file['weights']:
            state_dict[key] = torch.tensor(h5_file['weights'][key][()])

        model.load_state_dict(state_dict)

    return model, tokenizer, label_mapping

def integrate_with_fir_system():
    """
    Show how to integrate the trained model with the existing FIR system
    """
    # Train and save the model
    model_path = train_and_save_model()

    # Initialize the FIR system with the trained model
    from advanced_fir_classification_system import AdvancedFIRClassificationSystem

    # Custom initialization to use the trained model
    fir_system = AdvancedFIRClassificationSystem(model_name='bert-base-uncased')

    # Load the trained model
    model, tokenizer, label_mapping = load_model_from_h5(model_path)

    # Replace the default model in the FIR system
    fir_system.bert_model = model
    fir_system.tokenizer = tokenizer

    # Test the system with the trained model
    test_descriptions = [
        "Someone stole my wallet at the mall with a gun",
        "Domestic dispute with physical altercation",
        "Online banking fraud detected in my account",
        "Minor scuffle between neighbors, no serious injuries"
    ]

    for desc in test_descriptions:
        print("\nDescription:", desc)
        result = fir_system.schedule_case(desc)
        print("Case Details:")
        for key, value in result.items():
            print(f"{key.replace('_', ' ').title()}: {value}")

if __name__ == "__main__":
    # Run training and save model
    train_and_save_model()

    # Optional: Show integration with the FIR system
    # integrate_with_fir_system()

